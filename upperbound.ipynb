{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.36s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pycocotools.coco import COCO\n",
    "import json\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import pickle\n",
    "\n",
    "pickleFilePath = '../data/data/data/da_D0.pkl'\n",
    "pickleFilePath = '../data/data/data/ub_D0.pkl'\n",
    "pickleFile = open(pickleFilePath,'rb')\n",
    "pred_pkl = pickle.load(pickleFile)\n",
    "\n",
    "anno_path = '../data/data/bdd100k/annotations/det_val_D0.json'\n",
    "save_path = '../data/experiment/result.json'\n",
    "coco = COCO(anno_path)\n",
    "\n",
    "def get_pred_data_pkl(img_id):\n",
    "    index = img_id - 1\n",
    "    cls_score = pred_pkl[index]['pred_instances']['scores'].numpy()\n",
    "    bbox = pred_pkl[index]['pred_instances']['bboxes'].numpy()\n",
    "    cat_ids = pred_pkl[index]['pred_instances']['labels'].numpy()\n",
    "    return cat_ids,cls_score,bbox\n",
    "\n",
    "def get_gt_data(img_id):\n",
    "    anno_id = coco.getAnnIds(img_id)\n",
    "    anno_data = coco.loadAnns(anno_id)\n",
    "    bbox_list = []; cate_list = []\n",
    "    for anno in anno_data:\n",
    "        x1, y1, w, h = anno['bbox']\n",
    "        bbox_list.append([x1, y1, x1+w, y1+h])\n",
    "        cate_list.append(anno['category_id'])\n",
    "    bbox_data = np.array(bbox_list)\n",
    "    cate_data = np.array(cate_list)\n",
    "    return cate_data,bbox_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def calculate_iou(box1, box2):\n",
    "    # Calculate Intersection over Union (IoU) between two bounding boxes\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "\n",
    "    intersection_area = max(0, x2 - x1 + 1) * max(0, y2 - y1 + 1)\n",
    "    box1_area = (box1[2] - box1[0] + 1) * (box1[3] - box1[1] + 1)\n",
    "    box2_area = (box2[2] - box2[0] + 1) * (box2[3] - box2[1] + 1)\n",
    "    union_area = box1_area + box2_area - intersection_area\n",
    "\n",
    "    iou = intersection_area / union_area\n",
    "    return iou\n",
    "\n",
    "def calculate_ap(recall, precision):\n",
    "    # Calculate Average Precision (AP) given recall and precision arrays\n",
    "    recall = np.concatenate(([0.], recall, [1.]))\n",
    "    precision = np.concatenate(([0.], precision, [0.]))\n",
    "\n",
    "    for i in range(len(precision) - 1, 0, -1):\n",
    "        precision[i - 1] = max(precision[i - 1], precision[i])\n",
    "\n",
    "    indices = np.where(recall[1:] != recall[:-1])[0]\n",
    "    ap = np.sum((recall[indices + 1] - recall[indices]) * precision[indices + 1])\n",
    "    return ap\n",
    "\n",
    "def calculate_map(img_id,iou_threshold=0.5):\n",
    "    cat_id_gt, bbox_gt = get_gt_data(img_id)\n",
    "    cat_id_pred,scores_pred,bbox__pred = get_pred_data_pkl(img_id)\n",
    "    average_precisions = []\n",
    "    for index in range(10):\n",
    "        cid = index + 1\n",
    "        index_class_gt = np.where(cat_id_gt == cid)[0]\n",
    "        index_class_pred = np.where(cat_id_pred == cid)[0]\n",
    "        num_pred = index_class_pred.shape[0]\n",
    "        num_gt = index_class_gt.shape[0]\n",
    "\n",
    "        if num_pred == 0:\n",
    "            average_precisions.append(0)\n",
    "            continue\n",
    "\n",
    "        class_pred_score = scores_pred[index_class_pred]\n",
    "        class_pred_bbox = bbox__pred[index_class_pred]\n",
    "        class_pred_cid = cat_id_pred[index_class_pred]\n",
    "        class_gt_bbox = bbox_gt[index_class_gt]\n",
    "\n",
    "        class_sorted_indices = np.argsort(-class_pred_score)\n",
    "        class_pred_score_sorted = class_pred_score[class_sorted_indices]\n",
    "        class_pred_bbox_sorted = class_pred_bbox[class_sorted_indices]\n",
    "        class_pred_cid_sorted = class_pred_cid[class_sorted_indices]\n",
    "\n",
    "        true_positives = np.zeros(num_pred)\n",
    "        false_positives = np.zeros(num_pred)\n",
    "        matched_ground_truth = set()\n",
    "\n",
    "        for index_in_class in range(num_pred):\n",
    "            if num_gt == 0:\n",
    "                false_positives[index_in_class] = 1\n",
    "                continue\n",
    "\n",
    "            ious = [calculate_iou(class_pred_bbox_sorted[index_in_class], box) for box in class_gt_bbox]\n",
    "            max_iou = max(ious)\n",
    "            max_iou_index = np.argmax(ious)\n",
    "\n",
    "            if max_iou >= iou_threshold and max_iou_index not in matched_ground_truth:\n",
    "                true_positives[index_in_class] = 1\n",
    "                matched_ground_truth.add(max_iou_index)\n",
    "            else:\n",
    "                false_positives[index_in_class] = 1\n",
    "\n",
    "        true_positives = np.cumsum(true_positives)\n",
    "        false_positives = np.cumsum(false_positives)\n",
    "        recall = true_positives / num_gt\n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "\n",
    "        ap = calculate_ap(recall, precision)\n",
    "        average_precisions.append(ap)\n",
    "\n",
    "    mAP = np.mean(average_precisions)\n",
    "    return mAP\n",
    "\n",
    "\n",
    "#     # Calculate mean Average Precision (mAP) for object detection\n",
    "#     num_classes = len(predictions)\n",
    "#     average_precisions = []\n",
    "\n",
    "#     for class_id in range(num_classes):\n",
    "#         class_ground_truth = ground_truth[class_id]\n",
    "#         class_predictions = predictions[class_id]\n",
    "\n",
    "#         num_ground_truth = len(class_ground_truth)\n",
    "#         num_predictions = len(class_predictions)\n",
    "\n",
    "#         if num_predictions == 0:\n",
    "#             average_precisions.append(0)\n",
    "#             continue\n",
    "\n",
    "\n",
    "#         sorted_indices = np.argsort(-class_predictions[:, 4])\n",
    "#         class_predictions = class_predictions[sorted_indices]\n",
    "\n",
    "#         true_positives = np.zeros(num_predictions)\n",
    "#         false_positives = np.zeros(num_predictions)\n",
    "#         matched_ground_truth = set()\n",
    "\n",
    "#         for i in range(num_predictions):\n",
    "#             prediction = class_predictions[i]\n",
    "#             if num_ground_truth == 0:\n",
    "#                 false_positives[i] = 1\n",
    "#                 continue\n",
    "\n",
    "#             ious = [calculate_iou(prediction[:4], box) for box in class_ground_truth]\n",
    "#             max_iou = max(ious)\n",
    "#             max_iou_index = np.argmax(ious)\n",
    "\n",
    "#             if max_iou >= iou_threshold and max_iou_index not in matched_ground_truth:\n",
    "#                 true_positives[i] = 1\n",
    "#                 matched_ground_truth.add(max_iou_index)\n",
    "#             else:\n",
    "#                 false_positives[i] = 1\n",
    "\n",
    "#         true_positives = np.cumsum(true_positives)\n",
    "#         false_positives = np.cumsum(false_positives)\n",
    "#         recall = true_positives / num_ground_truth\n",
    "#         precision = true_positives / (true_positives + false_positives)\n",
    "\n",
    "#         ap = calculate_ap(recall, precision)\n",
    "#         average_precisions.append(ap)\n",
    "\n",
    "#     mAP = np.mean(average_precisions)\n",
    "#     return mAP"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 准备数据\n",
    "ground_truth_images = []  # 存储每张图片的 ground truth，可以是一个列表，每个元素表示一张图片的 ground truth\n",
    "predictions_images = []  # 存储每张图片的 predictions，可以是一个列表，每个元素表示一张图片的 predictions\n",
    "\n",
    "# 循环处理每张图片\n",
    "average_precisions = []\n",
    "for ground_truth, predictions in zip(ground_truth_images, predictions_images):\n",
    "    ap = calculate_map(ground_truth, predictions)\n",
    "    average_precisions.append(ap)\n",
    "\n",
    "# 计算 mAP\n",
    "mAP = np.mean(average_precisions)\n",
    "print(\"mAP:\", mAP)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=0.49s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=9.06s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=1.76s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.049\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.066\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.054\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.007\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.039\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.105\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.047\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.078\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.084\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.022\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.078\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.166\n"
     ]
    }
   ],
   "source": [
    "coco_result = []\n",
    "for sample_img in pred_pkl:\n",
    "    result = []\n",
    "    img_id = int(sample_img['img_id'])\n",
    "    ann_num = len(sample_img['pred_instances']['labels'])\n",
    "    for index in range(ann_num):\n",
    "        label = int(sample_img['pred_instances']['labels'][index])\n",
    "\n",
    "        label_map = [1,2,3,5,4,8,7,9,10,6]\n",
    "                   #[1,2,3,4,5,6,7,8, 9,10]\n",
    "        label = label_map[label-1]\n",
    "\n",
    "\n",
    "# {\"categories\": [{\"supercategory\": \"none\", \"id\": 1, \"name\": \"person\"}, {\"supercategory\": \"none\", \"id\": 2, \"name\": \"rider\"}, {\"supercategory\": \"none\", \"id\": 3, \"name\": \"car\"}, {\"supercategory\": \"none\", \"id\": 4, \"name\": \"bus\"}, {\"supercategory\": \"none\", \"id\": 5, \"name\": \"truck\"}, {\"supercategory\": \"none\", \"id\": 6, \"name\": \"bike\"}, {\"supercategory\": \"none\", \"id\": 7, \"name\": \"motor\"}, {\"supercategory\": \"none\", \"id\": 8, \"name\": \"traffic light\"}, {\"supercategory\": \"none\", \"id\": 9, \"name\": \"traffic sign\"}, {\"supercategory\": \"none\", \"id\": 10, \"name\": \"train\"}]\n",
    "\n",
    "\n",
    "        score = float(sample_img['pred_instances']['scores'][index])\n",
    "        bbox_xyxy = sample_img['pred_instances']['bboxes'][index].numpy().tolist()\n",
    "        if score < 0.04:\n",
    "            continue\n",
    "        xs = bbox_xyxy[0]\n",
    "        ys = bbox_xyxy[1]\n",
    "        ws = bbox_xyxy[2] - xs\n",
    "        hs = bbox_xyxy[3] - ys\n",
    "        bbox_xywh = [float(xs), float(ys), float(ws), float(hs)]   \n",
    "        result += [\n",
    "            {\n",
    "                'image_id': img_id,\n",
    "                'category_id': label,\n",
    "                'bbox': bbox_xywh,\n",
    "                'score': score\n",
    "            }\n",
    "        ]\n",
    "    # result = sorted(result, key=lambda x: x['score'])[-100:]\n",
    "    coco_result += result\n",
    "\n",
    "coco_dt = coco.loadRes(coco_result)\n",
    "coco_eval = COCOeval(coco, coco_dt,'bbox')\n",
    "# coco_eval.params.iouThrs = 0.05\n",
    "coco_eval.evaluate()\n",
    "coco_eval.accumulate()\n",
    "coco_eval.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = 200\n",
    "ann = coco_dt.getAnnIds(img)\n",
    "anndata = coco_dt.loadAnns(ann)\n",
    "\n",
    "ann_gt = coco.getAnnIds(img)\n",
    "anndata_gt = coco.loadAnns(ann_gt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 5, 5, 1, 1, 1, 5, 5, 5, 2, 3, 1, 1, 1, 1, 5, 5, 1, 5, 1, 1, 1, 5, 1, 1, 1, 1, 5, 1, 1, 3, 1, 1, 1, 1, 5, 5, 1, 1, 5, 1, 5, 5, 5, 1, 1, 1, 1, 1, 3, 5, 1, 3, 2, 1, 1, 3, 5, 5, 1, 5, 3, 5]\n",
      "[8, 8, 8, 8, 9, 9, 9, 3, 3, 3, 3, 3, 3, 3, 9, 9, 9]\n"
     ]
    }
   ],
   "source": [
    "xx1 = []\n",
    "xx2 = []\n",
    "for instance in anndata:\n",
    "    xx1.append(instance['category_id'])\n",
    "for instance in anndata_gt:\n",
    "    xx2.append(instance['category_id'])\n",
    "print(xx1)\n",
    "print(xx2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "9\n",
      "9\n",
      "9\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "9\n",
      "9\n",
      "9\n",
      "------------------\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "2\n",
      "3\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "1\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "# 定义画布尺寸\n",
    "canvas_width = 1280\n",
    "canvas_height = 720\n",
    "\n",
    "# 创建空白画布\n",
    "canvas = Image.new('RGB', (canvas_width, canvas_height), 'white')\n",
    "\n",
    "# 创建绘图对象\n",
    "draw = ImageDraw.Draw(canvas)\n",
    "for instance in anndata_gt:\n",
    "    bbox = instance['bbox']\n",
    "    print(instance['category_id'])\n",
    "    colors = [\n",
    "    (255, 0, 0),      # 红色\n",
    "    (0, 255, 0),      # 绿色\n",
    "    (0, 0, 255),      # 蓝色\n",
    "    (255, 255, 0),    # 黄色\n",
    "    (255, 0, 255),    # 品红色\n",
    "    (0, 255, 255),    # 青色\n",
    "    (128, 0, 0),      # 深红色\n",
    "    (0, 128, 0),      # 深绿色\n",
    "    (0, 0, 128),      # 深蓝色\n",
    "    (128, 128, 128)   # 灰色\n",
    "    ]\n",
    "    # 定义边界框数据\n",
    "    x = bbox[0]\n",
    "    y = bbox[1]\n",
    "    w = bbox[2]\n",
    "    h = bbox[3]\n",
    "    # 绘制边界框\n",
    "    draw.rectangle([(x, y), (x+w, y+h)], outline=colors[instance['category_id']-1], width=2)\n",
    "# 保存画布\n",
    "canvas.save('./img/output_gt.png')\n",
    "\n",
    "print('------------------')\n",
    "\n",
    "canvas1 = Image.new('RGB', (canvas_width, canvas_height), 'white')\n",
    "draw1 = ImageDraw.Draw(canvas1)\n",
    "for instance in anndata:\n",
    "    if instance['score'] < 0.10:\n",
    "        continue\n",
    "    bbox = instance['bbox']\n",
    "    print(instance['category_id'])\n",
    "    colors = [\n",
    "    (255, 0, 0),      # 红色\n",
    "    (0, 255, 0),      # 绿色\n",
    "    (0, 0, 255),      # 蓝色\n",
    "    (255, 255, 0),    # 黄色\n",
    "    (255, 0, 255),    # 品红色\n",
    "    (0, 255, 255),    # 青色\n",
    "    (128, 0, 0),      # 深红色\n",
    "    (0, 128, 0),      # 深绿色\n",
    "    (0, 0, 128),      # 深蓝色\n",
    "    (128, 128, 128)   # 灰色\n",
    "    ]\n",
    "    # 定义边界框数据\n",
    "    x = bbox[0]\n",
    "    y = bbox[1]\n",
    "    w = bbox[2]\n",
    "    h = bbox[3]\n",
    "    # 绘制边界框\n",
    "    draw1.rectangle([(x, y), (x+w, y+h)], outline=colors[instance['category_id']-1], width=2)\n",
    "# 保存画布\n",
    "canvas1.save('./img/output_pred.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
